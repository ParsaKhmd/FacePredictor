{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c5149d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e20fcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d5b447",
   "metadata": {},
   "source": [
    "Load Gender Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "025f27e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Load Gender Model\n",
    "gender_weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "gender_model = torchvision.models.efficientnet_b0(weights=gender_weights).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f44ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in gender_model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4748c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(1280, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(64, 1)\n",
    "\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2140f9",
   "metadata": {},
   "source": [
    "Load Age Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b97bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Load Age Model\n",
    "age_weights = torchvision.models.VGG19_Weights.DEFAULT\n",
    "age_model = torchvision.models.vgg19( weights=age_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "544bfeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters in the feature layers\n",
    "for param in age_model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze only the last 12 layers of the feature layers\n",
    "for param in age_model.features[-20:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dc92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_model.classifier = nn.Sequential(\n",
    "    \n",
    "    nn.Sequential(\n",
    "        nn.Linear(512 * 7 * 7, 4096),\n",
    "        nn.BatchNorm1d(4096),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),  \n",
    "        nn.Linear(4096, 4096),\n",
    "        nn.BatchNorm1d(4096),\n",
    "        nn.ReLU()\n",
    "    ),\n",
    "\n",
    "    \n",
    "    nn.Sequential(\n",
    "        nn.Linear(4096, 2048),\n",
    "        nn.BatchNorm1d(2048),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),  \n",
    "        nn.Linear(2048, 2048),\n",
    "        nn.BatchNorm1d(2048),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "    ),\n",
    "\n",
    "    \n",
    "    nn.Sequential(\n",
    "        nn.Linear(2048, 1024),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(1024, 1024),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "    ),\n",
    "\n",
    "    \n",
    "    nn.Sequential(\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),  \n",
    "        nn.Linear(512, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "    ),\n",
    "\n",
    "    \n",
    "    nn.Sequential(\n",
    "        nn.Linear(512, 256),\n",
    "        nn.BatchNorm1d(256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.1),  \n",
    "        nn.Linear(256, 256),\n",
    "        nn.BatchNorm1d(256),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "    ),\n",
    "\n",
    "    \n",
    "    nn.Linear(256, 1)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a95a1",
   "metadata": {},
   "source": [
    "Load Emotion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd726aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Load Emotion Model\n",
    "emotion_weights = torchvision.models.VGG19_Weights.DEFAULT\n",
    "emotion_model = torchvision.models.vgg19( weights=emotion_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0605f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters in the feature layers\n",
    "for param in emotion_model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in emotion_model.features[-20:].parameters():\n",
    "    param.requires_grad = True    \n",
    "\n",
    "\n",
    "# Modify the classifier\n",
    "emotion_model.classifier = nn.Sequential(\n",
    "    \n",
    "    nn.Sequential(\n",
    "        nn.Linear(512 * 7 * 7, 4096),\n",
    "        nn.BatchNorm1d(4096),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(4096, 4096),\n",
    "        nn.BatchNorm1d(4096),\n",
    "        nn.ReLU()\n",
    "    ),\n",
    "\n",
    "    \n",
    "    nn.Sequential(\n",
    "        nn.Linear(4096, 2048),\n",
    "        nn.BatchNorm1d(2048),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(2048, 2048),\n",
    "        nn.BatchNorm1d(2048),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "    ),\n",
    "\n",
    "    \n",
    "    nn.Sequential(\n",
    "        nn.Linear(2048, 1024),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(1024, 1024),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "    ),\n",
    "\n",
    "    \n",
    "    nn.Sequential(\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(512, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "    ),\n",
    "\n",
    "    \n",
    "    nn.Sequential(\n",
    "        nn.Linear(512, 256),\n",
    "        nn.BatchNorm1d(256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.BatchNorm1d(256),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "    ),\n",
    "\n",
    "    \n",
    "    nn.Linear(256, 7)  # 7 emotion classes\n",
    ").to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a01a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_model.load_state_dict(torch.load(\"fine_tuned_gender_model.pth\", map_location=device))\n",
    "gender_model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee72fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_model.load_state_dict(torch.load(\"fine_tuned_age_model.pth\", map_location=device))\n",
    "age_model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c97ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.load_state_dict(torch.load(\"emotion_model_fine_tuned.pth\", map_location=device))\n",
    "emotion_model.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb2f8b8",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe7f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation for gender model\n",
    "gender_infer_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e86043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation for age model\n",
    "age_infer_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad241364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation for emotion model\n",
    "emotion_infer_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a263d8",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591855af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_display(frame, face_box, gender_model, age_model, emotion_model):\n",
    "    # Extract face region from frame\n",
    "    x, y, w, h = face_box\n",
    "    face = frame[y:y+h, x:x+w]\n",
    "\n",
    "    # Convert to PIL image for model input\n",
    "    face_pil = Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Preprocess face for each model\n",
    "    gender_tensor = gender_infer_transform(face_pil).unsqueeze(0).to(device)\n",
    "    age_tensor = age_infer_transform(face_pil).unsqueeze(0).to(device)\n",
    "    emotion_tensor = emotion_infer_transform(face_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    # Run predictions\n",
    "    with torch.no_grad():\n",
    "        gender_output = gender_model(gender_tensor)\n",
    "        age_output = age_model(age_tensor)\n",
    "        emotion_output = emotion_model(emotion_tensor)\n",
    "        \n",
    "        # Process gender prediction\n",
    "        gender_pred = torch.round(torch.sigmoid(gender_output))\n",
    "        gender_index = int(gender_pred.item())\n",
    "        gender_labels = ['Male', 'Female']\n",
    "        gender_text = gender_labels[gender_index]\n",
    "        \n",
    "        # Process emotion prediction\n",
    "        age_text = int(age_output.item())\n",
    "        \n",
    "        # Process emotion prediction\n",
    "        emotion_index = torch.argmax(emotion_output, dim=1).item()\n",
    "        emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "        emotion_text = emotion_labels[emotion_index]\n",
    "\n",
    "    # Draw bounding box and label on frame\n",
    "    label = f\"Gender: {gender_text}, {age_text} years old, Emotion: {emotion_text}\"\n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    cv2.putText(frame, label, (x, y - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Initialize MediaPipe face detector\n",
    "    mp_face_detection = mp.solutions.face_detection\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    # Open webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    # Start face detection\n",
    "    with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.6) as face_detection:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Convert frame to RGB for MediaPipe\n",
    "            height, width, _ = frame.shape\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = face_detection.process(frame_rgb)\n",
    "\n",
    "            # If faces detected, process each\n",
    "            if results.detections:\n",
    "                for detection in results.detections:\n",
    "                    bbox = detection.location_data.relative_bounding_box\n",
    "                    x = int(bbox.xmin * width)\n",
    "                    y = int(bbox.ymin * height)\n",
    "                    w = int(bbox.width * width)\n",
    "                    h = int(bbox.height * height)\n",
    "\n",
    "                    # Clamp values to frame size\n",
    "                    x, y = max(0, x), max(0, y)\n",
    "                    w, h = min(w, width - x), min(h, height - y)\n",
    "\n",
    "                    frame = predict_and_display(frame, (x, y, w, h),\n",
    "                                                gender_model, age_model, emotion_model)\n",
    "                    \n",
    "            # Show the output\n",
    "            cv2.imshow('Webcam Face Analysis', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
